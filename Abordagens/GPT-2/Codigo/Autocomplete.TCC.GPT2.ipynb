{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSBpzlH6JtfE"
   },
   "source": [
    "# Generating suggestions for writing source code in C# language based on NLP.\n",
    "\n",
    "\n",
    "## GPT-2 approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhdi4xmdJtfI"
   },
   "source": [
    "#### This notebook was created and adapted for the work of generating suggestions using some ideas and codes as reference the notebook of the author \"Kayal, Arshabhi\" available at: \n",
    "https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNLwiFCOJtfJ"
   },
   "source": [
    "### Import and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQYuJ0ydJ4Tu",
    "outputId": "82cd2105-13ad-462b-a50a-8f6dcd0365d2"
   },
   "outputs": [],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNUT9X0nKGlN",
    "outputId": "6e2f94bc-5788-4c03-e512-f8faded0cd4e"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TiGkGUtaJtfJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath   \n",
    "from chardet import detect\n",
    "import nltk\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "from toolz import unique\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from pathlib import Path\n",
    "import codecs\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer, WEIGHTS_NAME, CONFIG_NAME\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, load_model\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQRj0TvQJtfK"
   },
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9rTe5mMjJtfK"
   },
   "outputs": [],
   "source": [
    "class BPE_token(object):\n",
    "    \"\"\"\n",
    "    Description: Used to create tokens in byte level.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Description: Initialize tokenizer object.\n",
    "        \"\"\"\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "        \n",
    "    def bpe_train(self, paths):\n",
    "        \"\"\"\n",
    "        Description: Train tokenizer to create tokens.\n",
    "        :param paths: List of file paths for files to read tokens.\n",
    "        \n",
    "        return: Void.\n",
    "        \"\"\"\n",
    "        trainer = BpeTrainer(show_progress=True, \n",
    "                             inital_alphabet=ByteLevel.alphabet(), \n",
    "                             special_tokens=[\"<s>\",\n",
    "                                             \"<pad>\",\n",
    "                                             \"</s>\",\n",
    "                                             \"<unk>\",\n",
    "                                             \"<mask>\"\n",
    "                                            ])\n",
    "        self.tokenizer.train(paths, trainer)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        \"\"\"\n",
    "        Description: Function to save generated tokens.\n",
    "        :param location: Directory to save generated tokens,\n",
    "        :param prefix: Prefix of file name. Default None.\n",
    "        \n",
    "        return: Void.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9f_QyWLJtfL"
   },
   "source": [
    "### Generic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fp9kcJl6JtfM"
   },
   "outputs": [],
   "source": [
    "def export_list_to_data_file(data, file_name):\n",
    "    \"\"\"\n",
    "    Description: Function to export data to data file.\n",
    "    :param data: Data to export,\n",
    "    :param file_name: File name to export.\n",
    "    \n",
    "    :return: Void.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_name, 'wb') as filehandle:\n",
    "        pickle.dump(data, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vE-oltwpJtfM"
   },
   "outputs": [],
   "source": [
    "def load_from_data_file(file_name):\n",
    "    \"\"\"\n",
    "    Description: Function to load data from file.\n",
    "    :param file_name: file name to load data from.\n",
    "    \n",
    "    :return: Type(list): List with data loaded from file.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    with open(file_name, 'rb') as filehandle:\n",
    "        data = pickle.load(filehandle)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "64PrbKDwJtfN"
   },
   "outputs": [],
   "source": [
    "def print_info(title, message = None, new_line = False):\n",
    "    \"\"\"\n",
    "    Description: Function to print info on screen.\n",
    "    :param title: Message title,\n",
    "    :param message: Message to print,\n",
    "    :param new_line: Indicates whether the first message will start with a line break or not.\n",
    "    \n",
    "    :return: Void.\n",
    "    \"\"\"\n",
    "    \n",
    "    if new_line:\n",
    "        print('\\n')\n",
    "    \n",
    "    print(\"####################################\")\n",
    "    print(title)\n",
    "    print(\"####################################\")\n",
    "    \n",
    "    if message:\n",
    "        print(\"%s\\n\" % (message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Dgyp37HsJtfO"
   },
   "outputs": [],
   "source": [
    "def get_sequence_of_numbers_from_string(str):\n",
    "    \"\"\"\n",
    "    Description: Function to extract all the sequence of numbers from the given string.\n",
    "    :param str: String to extract sequence of numbers.\n",
    "    \n",
    "    :return: Type(list): List with sequence of numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    array_numbers = re.findall(r'[0-9]+', str)\n",
    "    \n",
    "    return array_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ezMnFW4eJtfO"
   },
   "outputs": [],
   "source": [
    "def replace_sequence_of_numbers_for_mask(str_to_replace, \n",
    "                                         array_sequence_numbers_to_search, \n",
    "                                         mask_to_replace):\n",
    "    \"\"\"\n",
    "    Description: Function to replace sequence of numbers for specific mask.\n",
    "    :param str_to_replace: String to replace sequence of numbers,\n",
    "    :param array_sequence_numbers_to_search: Sequence numbers to search for,\n",
    "    :param mask_to_replace: Mask to replace each sequence.\n",
    "    \n",
    "    :return: Type(String): String with sequence of numbers replaced by mask.\n",
    "    \"\"\"\n",
    "    \n",
    "    for number_sequence in array_sequence_numbers_to_search:\n",
    "        str_to_replace = re.sub(str(number_sequence), mask_to_replace, str_to_replace, 1)\n",
    "\n",
    "    return str_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mA8qOTKBJtfO"
   },
   "outputs": [],
   "source": [
    "def get_encoding_type(file_path):\n",
    "    \"\"\"\n",
    "    Description: Function to retrieve enconding type of file.\n",
    "    :param file_path: File to get enconding.\n",
    "    \n",
    "    :return: Type(String): String with enconding type of file.\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(file_path, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VKBJ5UtWJtfP"
   },
   "outputs": [],
   "source": [
    "def change_enconding(source_file, enconding):\n",
    "    \"\"\"\n",
    "    Description: Function to change enconding of file.\n",
    "    :param source_file: File path to change enconding,\n",
    "    :param enconding: Enconding to replace in source_file.\n",
    "    \n",
    "    :return: Void.\n",
    "    \"\"\"\n",
    "    \n",
    "    from_codec = get_encoding_type(source_file)\n",
    "    \n",
    "    try: \n",
    "        target_file = source_file.replace(ntpath.basename(source_file), \"123%s\" % (ntpath.basename(source_file))) \n",
    "        \n",
    "        with open(source_file, 'r', encoding=from_codec) as f, open(target_file, 'w', encoding=enconding) as e:\n",
    "                text = f.read()\n",
    "                e.write(text)\n",
    "                f.close()\n",
    "\n",
    "        os.remove(source_file) \n",
    "        os.rename(target_file, source_file) \n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Decode error for file: '%s'\" % (source_file))\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"Encode error for file: '%s'\" % (source_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qJUSfsz5JtfP"
   },
   "outputs": [],
   "source": [
    "def check_utf8_encode(file_name):\n",
    "    \"\"\"\n",
    "    Description: Function to check if file has UTF-8 encoding.\n",
    "    :param file_name: File path to check UTF-8 encoding.\n",
    "    \n",
    "    :return: Type(Bool) True: UTF-8 Encoding, False: Not UTF-8 Encoding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = codecs.open(file_name, encoding=\"utf-8\", errors=\"strict\").readlines()\n",
    "\n",
    "        if content is not None:\n",
    "            return True\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F7tDGif5JtfP"
   },
   "outputs": [],
   "source": [
    "def flatten_list(list_to_flatten):\n",
    "    \"\"\"\n",
    "    Description: Function to flatten the given list.\n",
    "    :param list_to_flatten: List to flatten.\n",
    "    \n",
    "    :return: Type(List): Flat list.\n",
    "    \"\"\"   \n",
    "    \n",
    "    return [f for child_list in list_to_flatten for f in child_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nMzuLxV5JtfQ"
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_items_from_list(list_to_remove_duplicates):\n",
    "    \"\"\"\n",
    "    Description: Function to remove duplicate itens from given list.\n",
    "    :param list_to_remove_duplicates: List to remove duplicates.\n",
    "    \n",
    "    :return: Type(List): List without duplicates.\n",
    "    \"\"\"  \n",
    "    \n",
    "    return list(map(list, unique(map(tuple, list_to_remove_duplicates))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PZfBV9PJtfQ"
   },
   "source": [
    "### C# repository functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y2RCu6cAJtfQ"
   },
   "outputs": [],
   "source": [
    "def get_all_c_sharp_complete_file_names_for_each_class(root_directory):\n",
    "    \"\"\"\n",
    "    Description: Function to get all complete name of files with extension \".cs\" (C# class).\n",
    "    :param root_directory: Root directory of files.\n",
    "    \n",
    "    :return: Type(List): List with all file names of C# repository.\n",
    "    \"\"\"\n",
    "    \n",
    "    C_SHARP_CLASS_FILE_EXTENSION = \".cs\"\n",
    "    \n",
    "    complete_name_of_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(C_SHARP_CLASS_FILE_EXTENSION):\n",
    "                complete_name_of_files.append(os.path.join(root, file))\n",
    "    \n",
    "    return complete_name_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0BUM3tYJtfQ"
   },
   "source": [
    "### Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QCcB2tTHJtfR"
   },
   "outputs": [],
   "source": [
    "def tokenize_all_files(complete_file_names, path_to_save_tokens):\n",
    "    \"\"\"\n",
    "    Description: Function to tokenize all files and save into specific folder.\n",
    "    :param complete_file_names: All C# files list (Name of each file),\n",
    "    :param path_to_save_tokens: Path to save generated tokens.\n",
    "    \n",
    "    :returns - Void.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = BPE_token()\n",
    "\n",
    "    tokenizer.bpe_train([c for c in complete_file_names if check_utf8_encode(c) == True])\n",
    "\n",
    "    tokenizer.save_tokenizer(path_to_save_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ungzqw7rJtfR"
   },
   "source": [
    "### GPT-2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xqapeVEaJtfR"
   },
   "outputs": [],
   "source": [
    "def create_gpt2_model_and_load_tokenizer(tokens_path):\n",
    "    \"\"\"\n",
    "    Description: Function to create GPT-2 model and load tokenizer saved previously.\n",
    "    :param tokens_path: Directory from generated tokens.\n",
    "    \n",
    "    :return: Type(TFGPT2LMHeadModel, GPT2Tokenizer) - model and tokenizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokens_path)\n",
    "    tokenizer.add_special_tokens({\n",
    "      \"eos_token\": \"</s>\",\n",
    "      \"bos_token\": \"<s>\",\n",
    "      \"unk_token\": \"<unk>\",\n",
    "      \"pad_token\": \"<pad>\",\n",
    "      \"mask_token\": \"<mask>\"\n",
    "    })\n",
    "\n",
    "    config = GPT2Config(\n",
    "      vocab_size=tokenizer.vocab_size,\n",
    "      bos_token_id=tokenizer.bos_token_id,\n",
    "      eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    model = TFGPT2LMHeadModel(config)\n",
    "    \n",
    "    return (model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_CTlsA7_JtfR"
   },
   "outputs": [],
   "source": [
    "def create_string_list_tokens(complete_file_names, tokenizer):\n",
    "    \"\"\"\n",
    "    Description: Function to create tokens list from tokenizer.\n",
    "    :param complete_file_names: List of file paths from all c# files,\n",
    "    :param tokenizer: Tokenizer object.\n",
    "    \n",
    "    :return: Type(String) - Single string tokenized.\n",
    "    \"\"\"\n",
    "    \n",
    "    single_string = ''\n",
    "    \n",
    "    for filename in complete_file_names:\n",
    "        with open(file_name, \"r\", encoding='utf-8') as f:\n",
    "            if check_utf8_encode(file_name) == True:\n",
    "                x = f.read()\n",
    "                single_string += x + tokenizer.eos_token\n",
    "\n",
    "    return tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "56oDT2GfJtfS"
   },
   "outputs": [],
   "source": [
    "def create_tf_dataset_for_gpt2_training(tokens_list):\n",
    "    \"\"\"\n",
    "    Description: Function to create Tensor Flow dataset for training.\n",
    "    :param tokens_list: List of tokens.\n",
    "    \n",
    "    :return: Type(TF.Dataset) - Tensor Flow dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    examples = []\n",
    "    block_size = 100\n",
    "    BATCH_SIZE = 12\n",
    "    BUFFER_SIZE = 1000\n",
    "    \n",
    "    for i in range(0, len(tokens_list) - block_size + 1, block_size):\n",
    "        examples.append(tokens_list[i:i + block_size])\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    \n",
    "    for ex in examples:\n",
    "        inputs.append(ex[:-1])\n",
    "        labels.append(ex[1:])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GRbz3vbQJtfS"
   },
   "outputs": [],
   "source": [
    "def config_model_to_gpt2(model):\n",
    "    \"\"\"\n",
    "    Description: Function to configure GPT-2 model for training.\n",
    "    :param model: GPT-2 model.\n",
    "    \n",
    "    :return: Type(TFGPT2LMHeadModel) - Configured GPT-2 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZuxLPQes2Ow3"
   },
   "outputs": [],
   "source": [
    "def create_checkpoint_gpt2(filepath_to_save_checkpoint):\n",
    "    \"\"\"\n",
    "    Description: Function to create chekpoint for GPT-2 training.\n",
    "    :param filepath_to_save_checkpoint: File path to save checkpoint.\n",
    "    \n",
    "    :return: Type(Callback) - Call back list for each epoch checkpoint generated.\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath_to_save_checkpoint, monitor='logits_accuracy', verbose=1, save_best_only=True)\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3BhIdrc1JtfS"
   },
   "outputs": [],
   "source": [
    "def train_model_to_gpt2(model, dataset, num_epoch, batch_size, callbacks_list):\n",
    "    \"\"\"\n",
    "    Description: Function to train GPT-2 model.\n",
    "    :param model: Model to train,\n",
    "    :param dataset: Dataset with data to use in training,\n",
    "    :param num_epoch: Number of training epochs,\n",
    "    :param batch_size: Batch size of training,\n",
    "    :param callbacks_list: Callback list for save weights in each completed epoch.\n",
    "    \n",
    "    :return: Type(History) - History with results of training.\n",
    "    \"\"\"    \n",
    "\n",
    "    history = model.fit(dataset, epochs=num_epoch, batch_size=batch_size, callbacks=callbacks_list)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tNukOEW8JtfS"
   },
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, model, num_return_sequences, tokenizer):\n",
    "    \"\"\"\n",
    "    Description: Function to get suggestions from trained model.\n",
    "    :param previous_tokens: Previous token to get suggestions,\n",
    "    :param model: Trained model,\n",
    "    :param num_return_sequences: How many sequences (suggestions) this function should return,\n",
    "    :param tokenizer: GPT-2 Tokenizer to decode suggestions.\n",
    "    \n",
    "    :return: Type(List) - List with generated suggestions.\n",
    "    \"\"\"   \n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    input_ids = tokenizer.encode(previous_tokens, return_tensors='tf')\n",
    "    \n",
    "    beam_output = model.generate(\n",
    "      input_ids,\n",
    "      max_length = 50,\n",
    "      num_beams = 1,\n",
    "      temperature = 0.3,\n",
    "      no_repeat_ngram_size = 0,\n",
    "      num_return_sequences = num_return_sequences,\n",
    "\n",
    "    )\n",
    "\n",
    "    for i in range(len(beam_output)):\n",
    "        suggestions.append(tokenizer.decode(beam_output[i], skip_special_tokens = True))\n",
    "\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LG3ogdjylIrD"
   },
   "outputs": [],
   "source": [
    "def save_model_to_external_file(folder_to_save_model, model_to_save):\n",
    "    \"\"\"\n",
    "    Description: Function to save trained model.\n",
    "    :param folder_to_save_model: Directory to save model,\n",
    "    :param model_to_save: Trained model to save.\n",
    "    \n",
    "    :return: Void.\n",
    "    \"\"\"  \n",
    "    \n",
    "    output_dir = './' + folder_to_save_model + '/'\n",
    "    \n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    \n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    \n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    model_to_save.config.to_json_file(output_config_file)\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BY5IgTiT_0u"
   },
   "source": [
    "### Format result for possible code that compiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OqbFeI_vfsI-"
   },
   "outputs": [],
   "source": [
    "def remove_comments_from_suggestion(suggestion):\n",
    "    \"\"\"\n",
    "    Description: Function to remove C# comments from suggestions.\n",
    "    :param suggestion: String suggestion to remove comments.\n",
    "    \n",
    "    :return: Type(String) - Suggestion without the first line comment.\n",
    "    \"\"\"  \n",
    "    \n",
    "    suggestion_formated = \"\"\n",
    "    suggestion_line_by_line = suggestion.split('\\n')\n",
    "\n",
    "    for s in suggestion_line_by_line:\n",
    "\n",
    "        if not(s.startswith(\"//\") or s.startswith(\"/*\") or s.endswith(\"*/\")):\n",
    "            suggestion_formated += s + \"\\n\"\n",
    "\n",
    "    return suggestion_formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "baSpQQa62ESo"
   },
   "outputs": [],
   "source": [
    "def format_suggestion(previous_tokens, suggestion):\n",
    "    \"\"\"\n",
    "    Description: Function to format generated suggestion.\n",
    "    :param previous_tokens: Previous tokens from suggestion,\n",
    "    :param suggestion: Suggestion to format.\n",
    "    \n",
    "    :return: Type(String) - Formated suggestion.\n",
    "    \"\"\"  \n",
    "    \n",
    "    try:\n",
    "        suggestion = remove_comments_from_suggestion(suggestion)\n",
    "        return suggestion.split('\\n')[0]\n",
    "\n",
    "    except:\n",
    "        # Invalid suggestion\n",
    "        return \"\"\n",
    "\n",
    "    # Invalid suggestion\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8pq1lUxmmvC"
   },
   "source": [
    "### Generate common C# data to test suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ERXQJmqzmmaG"
   },
   "outputs": [],
   "source": [
    "def load_csv_to_test_previous_tokens_list(csv_file_path):\n",
    "    \"\"\"\n",
    "    Description: Function to load data from csv file to previous tokens list.\n",
    "    :param csv_file_path: Csv file path with data for previous tokens .\n",
    "    \n",
    "    :return: Type(List): List with previous tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe_tokens_test = pd.read_csv(csv_file_path, delimiter=\";\", header=None)\n",
    "\n",
    "    previous_tokens_list = []\n",
    "\n",
    "    for index, row in dataframe_tokens_test.iterrows():\n",
    "        token = \"\"\n",
    "        for column in range(len(dataframe_tokens_test.columns.tolist())):\n",
    "            if type(row[column]) == str:\n",
    "                token += row[column] + \" \"\n",
    "\n",
    "        previous_tokens_list.append(token[:-1])\n",
    "\n",
    "    return previous_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7xdItzzDVWzA"
   },
   "outputs": [],
   "source": [
    "def save_suggestions_to_csv_file(file_path_csv, all_suggestions):\n",
    "    \"\"\"\n",
    "    Description: Function to save suggestions into csv file.\n",
    "    :param csv_file_path: Csv file path to save suggestions,\n",
    "    :param all_suggestions: List with all sugestions to save in csv file.\n",
    "    \n",
    "    :return: Void.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_to_save_in_csv = []\n",
    "\n",
    "    for suggestions in all_suggestions:\n",
    "        for i in range(len(suggestions[1])):\n",
    "            data_to_save_in_csv.append([suggestions[0], format_suggestion(suggestions[0], suggestions[1][i])])\n",
    "\n",
    "    with open(file_path_csv, 'w', newline='') as f:\n",
    "      \n",
    "        write = csv.writer(f, delimiter=\";\")      \n",
    "        write.writerows(data_to_save_in_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPNzoK9tJtfT"
   },
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdOikWDT3wQ8"
   },
   "source": [
    "#### Mounting Google Drive (For Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QvZAotO2ax_",
    "outputId": "fac97d17-ef8c-48bf-da28-808a6448b832"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZlxq4tD9irz"
   },
   "source": [
    "#### Create constants for directories (Local Jupyter Notebook or Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBtDSZbx9rC6"
   },
   "outputs": [],
   "source": [
    "PATH_TO_TOKENS_LIST = '/'\n",
    "DIRECTORY_GENERATED_TOKENS = ''\n",
    "PATH_CHECKPOINT_TRAIN = ''\n",
    "DIRECTORY_TF_DATASET = ''\n",
    "PATH_TO_TEST_FILE_1_WORD = ''\n",
    "PATH_TO_TEST_FILE_2_WORD = ''\n",
    "PATH_TO_TEST_FILE_3_WORD = ''\n",
    "PATH_TO_TEST_FILE_4_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_1_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_2_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_3_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_4_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_DATA_1_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_DATA_2_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_DATA_3_WORD = ''\n",
    "PATH_TO_SAVE_SUGGESTIONS_DATA_4_WORD = ''\n",
    "\n",
    "executing_in_colab = True\n",
    "\n",
    "if executing_in_colab == True:\n",
    "    PATH_TO_TOKENS_LIST = '/content/drive/MyDrive/TCC/GPT2/tokens_list.data'\n",
    "    DIRECTORY_GENERATED_TOKENS = '/content/drive/MyDrive/TCC/GPT2/'\n",
    "    PATH_CHECKPOINT_TRAIN = '/content/drive/MyDrive/TCC/GPT2/CheckPoint'\n",
    "    DIRECTORY_TF_DATASET = '/content/drive/MyDrive/TCC/GPT2/TFDataset'\n",
    "    PATH_TO_TEST_FILE_1_WORD = '/content/drive/MyDrive/TCC/TestFiles/previous_tokens_for_test_1_word.csv'\n",
    "    PATH_TO_TEST_FILE_2_WORD = '/content/drive/MyDrive/TCC/TestFiles/previous_tokens_for_test_2_word.csv'\n",
    "    PATH_TO_TEST_FILE_3_WORD = '/content/drive/MyDrive/TCC/TestFiles/previous_tokens_for_test_3_word.csv'\n",
    "    PATH_TO_TEST_FILE_4_WORD = '/content/drive/MyDrive/TCC/TestFiles/previous_tokens_for_test_4_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_1_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_1_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_2_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_2_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_3_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_3_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_4_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_4_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_1_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_1_word.data'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_2_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_2_word.data'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_3_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_3_word.data'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_4_WORD = '/content/drive/MyDrive/TCC/TestFiles/suggestions_4_word.data'\n",
    "else:\n",
    "    PATH_TO_TOKENS_LIST = 'tokens_list.data'\n",
    "    DIRECTORY_GENERATED_TOKENS = 'GPT2_Generated_Tokens'\n",
    "    PATH_CHECKPOINT_TRAIN = 'CheckPoint'\n",
    "    DIRECTORY_TF_DATASET = 'TFDataset'\n",
    "    PATH_TO_TEST_FILE_1_WORD = 'previous_tokens_for_test_1_word.csv'\n",
    "    PATH_TO_TEST_FILE_2_WORD = 'previous_tokens_for_test_2_word.csv'\n",
    "    PATH_TO_TEST_FILE_3_WORD = 'previous_tokens_for_test_3_word.csv'\n",
    "    PATH_TO_TEST_FILE_4_WORD = 'previous_tokens_for_test_4_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_1_WORD = 'suggestions_1_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_2_WORD = 'suggestions_2_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_3_WORD = 'suggestions_3_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_4_WORD = 'suggestions_4_word.csv'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_1_WORD = 'suggestions_1_word.data'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_2_WORD = 'suggestions_2_word.data'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_3_WORD = 'suggestions_3_word.data'\n",
    "    PATH_TO_SAVE_SUGGESTIONS_DATA_4_WORD = 'suggestions_4_word.data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4b1n2dPJtfT"
   },
   "source": [
    "#### Read and filter C# class files (.cs) from root repository downladed from: https://github.com/dotnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziA_Gw8DJtfT"
   },
   "outputs": [],
   "source": [
    "# Define constants.\n",
    "ROOT_DIRECTORY = \"D:\\DsTCC\"\n",
    "\n",
    "# Get all file names.\n",
    "complete_file_names = get_all_c_sharp_complete_file_names_for_each_class(ROOT_DIRECTORY)\n",
    "\n",
    "# Print first 10 files.\n",
    "print_info(\"First 10 files:\")\n",
    "\n",
    "for file_name in complete_file_names[:10]:\n",
    "    print(ntpath.basename(file_name)) \n",
    "\n",
    "# Print total number of files.\n",
    "print_info(\"Number of files for GPT-2:\", new_line=True)\n",
    "print(\"%s files.\" % (len(complete_file_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8qHax6iJtfU"
   },
   "source": [
    "#### Tokenize all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzmDcOObJtfU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize_all_files(complete_file_names, DIRECTORY_GENERATED_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiF5RlvHJtfU"
   },
   "source": [
    "#### Create GPT-2 model and vocabulary for tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSFokriMJtfU",
    "outputId": "fa077602-bb2c-497b-ab6d-b9de0c199cd7"
   },
   "outputs": [],
   "source": [
    "(model, tokenizer) = create_gpt2_model_and_load_tokenizer(DIRECTORY_GENERATED_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeNF6iSJJtfV"
   },
   "source": [
    "#### Create tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXFEB5ZNJtfV"
   },
   "outputs": [],
   "source": [
    "tokens_list = create_string_list_tokens(complete_file_names, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE9IeCe9ajkm"
   },
   "source": [
    "#### Export tokens_list to backup file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pShM4BCtJtfV"
   },
   "outputs": [],
   "source": [
    "export_list_to_data_file(tokens_list, 'tokens_list.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8TBHx4BavVh"
   },
   "source": [
    "#### Load tokens_list from backup file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fbq6VJHEJtfV"
   },
   "outputs": [],
   "source": [
    "tokens_list = load_from_data_file(PATH_TO_TOKENS_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6ex0F0ba4Ra"
   },
   "source": [
    "#### Show words positions of created list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzFvZPzDJtfV"
   },
   "outputs": [],
   "source": [
    "print_info(\"First 5 tokens positions from list:\")\n",
    "\n",
    "for token in tokens_list[:5]:\n",
    "    print(\"Position in vocabulary: %s\" % (token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crcyF35OJtfV"
   },
   "source": [
    "#### Create tensor flow dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYHwY5ZcJtfV"
   },
   "outputs": [],
   "source": [
    "dataset = create_tf_dataset_for_gpt2_training(tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWCzYgV-_Yal"
   },
   "source": [
    "#### Export tensorflow dataset to backup file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwrnv2kIqUAQ"
   },
   "outputs": [],
   "source": [
    "tf.data.experimental.save(dataset, DIRECTORY_TF_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEK3y84sdVcy"
   },
   "source": [
    "#### Load tensorflow dataset from backup file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDFxSNgkdZDw"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.experimental.load(DIRECTORY_TF_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EatRAf-JtfW"
   },
   "source": [
    "#### Configure model and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGBx6eesJtfW"
   },
   "outputs": [],
   "source": [
    "model = config_model_to_gpt2(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fBb7mxqJtfW"
   },
   "source": [
    "#### Train model with checkpoint in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0p0azUDJtfW"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 5\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "callbacks_list = create_checkpoint_gpt2(PATH_CHECKPOINT_TRAIN)\n",
    "\n",
    "history = train_model_to_gpt2(model, dataset, NUM_EPOCH, BATCH_SIZE, callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7omhSPAzl9Wp"
   },
   "source": [
    "#### Export model to backup files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "as43xTM6lvwl"
   },
   "outputs": [],
   "source": [
    "FOLDER_TO_SAVE_MODEL = 'model_backup'\n",
    "save_model_to_external_file(FOLDER_TO_SAVE_MODEL, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5s0v01Jdbgyy"
   },
   "source": [
    "#### Load model from backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qGgwh4-bkmJ",
    "outputId": "15681564-3c92-4cd3-d47e-906df8a6fe92"
   },
   "outputs": [],
   "source": [
    "model.load_weights(PATH_CHECKPOINT_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn-9IDTcJtfW"
   },
   "source": [
    "#### Get suggestions for the most common C# word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUv8N_NzJtfW",
    "outputId": "a70e3ce2-05f5-4bd9-f4ca-9affb33737ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Suggestion 1:\n",
      "####################################\n",
      "private static void ResolveExecutablePath(ref string executable, ref string args)\n",
      "        {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_tokens = 'private static void'\n",
    "num_return_sequences = 1\n",
    "suggestions = get_suggestions(previous_tokens, model, num_return_sequences, tokenizer)\n",
    "\n",
    "for i in range(len(suggestions)):\n",
    "    print_info(\"Suggestion %s:\" % (i + 1), format_suggestion(previous_tokens, suggestions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUEaGTca3GtY"
   },
   "source": [
    "### Get suggestions for 1 word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdCfq_-gBHgS"
   },
   "outputs": [],
   "source": [
    "all_suggestions_1_word = load_from_data_file(PATH_TO_SAVE_SUGGESTIONS_DATA_1_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hD1IpmaQuHav"
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 1\n",
    "previous_tokens_to_test = load_csv_to_test_previous_tokens_list(PATH_TO_TEST_FILE_1_WORD)\n",
    "\n",
    "all_suggestions_1_word = []\n",
    "\n",
    "for previous_tokens in previous_tokens_to_test:\n",
    "    all_suggestions_1_word.append((previous_tokens, get_suggestions(previous_tokens, model, num_return_sequences, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cGFKhKwJFpq"
   },
   "outputs": [],
   "source": [
    "save_suggestions_to_csv_file(PATH_TO_SAVE_SUGGESTIONS_1_WORD, all_suggestions_1_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbKZ74yn00MV"
   },
   "outputs": [],
   "source": [
    "export_list_to_data_file(all_suggestions_1_word, PATH_TO_SAVE_SUGGESTIONS_DATA_1_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjXuqIG_DYMb",
    "outputId": "9df037bc-ed53-4cc7-dd2e-ce1f4de255ec"
   },
   "outputs": [],
   "source": [
    "for suggestions in all_suggestions_1_word:\n",
    "    print(\"###############################################################################\")\n",
    "    print(\"Previous tokens: \" + suggestions[0])\n",
    "    for i in range(len(suggestions[1])):\n",
    "        print(\"Suggestion: \" + format_suggestion(suggestions[0], suggestions[1][i]))\n",
    "        print(\" \")\n",
    "\n",
    "    print(\"###############################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I00wZBMi2EKf"
   },
   "source": [
    "### Get suggestions for 2 word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnBdL1YgFL8Q"
   },
   "outputs": [],
   "source": [
    "all_suggestions_2_word = load_from_data_file(PATH_TO_SAVE_SUGGESTIONS_DATA_2_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Rw5RIa72QRn"
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 1\n",
    "previous_tokens_to_test = load_csv_to_test_previous_tokens_list(PATH_TO_TEST_FILE_2_WORD)\n",
    "\n",
    "all_suggestions_2_word = []\n",
    "\n",
    "for previous_tokens in previous_tokens_to_test:\n",
    "    all_suggestions_2_word.append((previous_tokens, get_suggestions(previous_tokens, model, num_return_sequences, tokenizer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F410bL3Q1p3l"
   },
   "outputs": [],
   "source": [
    "export_list_to_data_file(all_suggestions_2_word, PATH_TO_SAVE_SUGGESTIONS_DATA_2_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDQlAx5y1p3l"
   },
   "outputs": [],
   "source": [
    "save_suggestions_to_csv_file(PATH_TO_SAVE_SUGGESTIONS_2_WORD, all_suggestions_2_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRtDiUDQ1p3d",
    "outputId": "9cf3e8af-d8c1-4fdf-e4e2-95b9e7a5f48f"
   },
   "outputs": [],
   "source": [
    "for suggestions in all_suggestions_2_word:\n",
    "    print(\"###############################################################################\")\n",
    "    print(\"Previous tokens: \" + suggestions[0])\n",
    "    for i in range(len(suggestions[1])):\n",
    "        print(\"Suggestion: \" + format_suggestion(suggestions[0], suggestions[1][i]))\n",
    "        print(\" \")\n",
    "\n",
    "    print(\"###############################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ8lOFAN2jtm"
   },
   "source": [
    "### Get suggestions for 3 word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRlT4DxeFagM"
   },
   "outputs": [],
   "source": [
    "all_suggestions_3_word = load_from_data_file(PATH_TO_SAVE_SUGGESTIONS_DATA_3_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmdAy1Fa2jtu"
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 1\n",
    "previous_tokens_to_test = load_csv_to_test_previous_tokens_list(PATH_TO_TEST_FILE_3_WORD)\n",
    "\n",
    "all_suggestions_3_word = []\n",
    "\n",
    "for previous_tokens in previous_tokens_to_test:\n",
    "    all_suggestions_3_word.append((previous_tokens, get_suggestions(previous_tokens, model, num_return_sequences, tokenizer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4irovUe12jtv"
   },
   "outputs": [],
   "source": [
    "save_suggestions_to_csv_file(PATH_TO_SAVE_SUGGESTIONS_3_WORD, all_suggestions_3_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7739Z-kX2jtv"
   },
   "outputs": [],
   "source": [
    "export_list_to_data_file(all_suggestions_3_word, PATH_TO_SAVE_SUGGESTIONS_DATA_3_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OjQeBLe2jtu",
    "outputId": "69053817-0b74-40cc-f340-4c086e473a8a"
   },
   "outputs": [],
   "source": [
    "for suggestions in all_suggestions_3_word:\n",
    "    print(\"###############################################################################\")\n",
    "    print(\"Previous tokens: \" + suggestions[0])\n",
    "    for i in range(len(suggestions[1])):\n",
    "        print(\"Suggestion: \" + format_suggestion(suggestions[0], suggestions[1][i]))\n",
    "        print(\" \")\n",
    "\n",
    "    print(\"###############################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsLit3mB2zng"
   },
   "source": [
    "### Get suggestions for 4 word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFu4TParFq0y"
   },
   "outputs": [],
   "source": [
    "all_suggestions_4_word = load_from_data_file(PATH_TO_SAVE_SUGGESTIONS_DATA_4_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgLXj5z52znh"
   },
   "outputs": [],
   "source": [
    "num_return_sequences = 1\n",
    "previous_tokens_to_test = load_csv_to_test_previous_tokens_list(PATH_TO_TEST_FILE_4_WORD)\n",
    "\n",
    "all_suggestions_4_word = []\n",
    "\n",
    "for previous_tokens in previous_tokens_to_test:\n",
    "    all_suggestions_4_word.append((previous_tokens, get_suggestions(previous_tokens, model, num_return_sequences, tokenizer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hBw6gw12znh"
   },
   "outputs": [],
   "source": [
    "save_suggestions_to_csv_file(PATH_TO_SAVE_SUGGESTIONS_4_WORD, all_suggestions_4_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glfKzAD42znh"
   },
   "outputs": [],
   "source": [
    "export_list_to_data_file(all_suggestions_4_word, PATH_TO_SAVE_SUGGESTIONS_DATA_4_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0r9C_xuF2znh",
    "outputId": "aebed432-d058-403a-f86f-a78313fb4d44"
   },
   "outputs": [],
   "source": [
    "for suggestions in all_suggestions_4_word:\n",
    "    print(\"###############################################################################\")\n",
    "    print(\"Previous tokens: \" + suggestions[0])\n",
    "    for i in range(len(suggestions[1])):\n",
    "        print(\"Suggestion: \" + format_suggestion(suggestions[0], suggestions[1][i]))\n",
    "        print(\" \")\n",
    "\n",
    "    print(\"###############################################################################\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Autocomplete_TCC_GPT2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
